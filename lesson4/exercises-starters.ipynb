{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "This is the notebook containing the exercises for Feature Store, Model Monitor, and Clarify. Tested for these exercises was performed using __2 vCPU + 4 GiB notebook instance with Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized) kernel__.\n",
    "\n",
    "## Staging\n",
    "\n",
    "We'll begin by initializing some variables. These are often assumed to be present in code samples you'll find in the AWS documenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to load JumpStart region config.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/jumpstart/constants.py\", line 69, in _load_region_config\n",
      "    with open(filepath) as f:\n",
      "         ^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/jumpstart/region_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature store is a special database to give ML systems a consistent data flow across training and inference workloads. It can ingest data in batches (for training) as well as serve input features to models with very low latency for real-time prediction.\n",
    "\n",
    "For this exercise we'll work with a wine quality dataset: https://archive.ics.uci.edu/ml/datasets/wine+quality/\n",
    "\n",
    "```P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import time\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we leave the column names as-is, Feature Store won't be able to handle the `/` in `od280/od315_of_diluted_wines` (`/` is a delimiter Feature Store uses to manage how features are organized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data, we can create a feature group. Remember to attach event time and ID columns - Feature Store needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='alcohol', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='malic_acid', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='ash', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='alcalinity_of_ash', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='magnesium', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='total_phenols', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='flavanoids', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='nonflavanoid_phenols', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='proanthocyanins', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='color_intensity', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='hue', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='od280_od315_of_diluted_wines', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='proline', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='EventTime', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a feature group\n",
    "df[\"EventTime\"] = time.time()\n",
    "df[\"id\"] = range(len(df))\n",
    "\n",
    "# Create feature group\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    name=\"wine_quality\", sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Load Feature definitions\n",
    "feature_group.load_feature_definitions(data_frame=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature group is not created until we call the `create` method, let's do that now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:2                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 # create feature store</span>                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"font-weight: bold; text-decoration: underline\">feature_group.create(</span>                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">s3_uri=</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">f\"s3://{</span><span style=\"font-weight: bold; text-decoration: underline\">bucket</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">}/features\"</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">record_identifier_name = </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">'id'</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">event_time_feature_name = </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"EventTime\"</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/feature_st</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ore/</span><span style=\"font-weight: bold\">feature_group.py</span>:789 in create                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 786 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>{<span style=\"color: #808000; text-decoration-color: #808000\">\"offline_store_config\"</span>: offline_store_config.to_dict()}                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 787 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 788 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 789 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.create_feature_group(**create_feature_store_args)</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 790 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 791 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">delete</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 792 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Delete a FeatureGroup.\"\"\"</span>                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/</span><span style=\"font-weight: bold\">session.py</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> :6614 in create_feature_group                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6611 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>Description=description,                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6612 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>Tags=tags,                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6613 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>6614 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_client.create_feature_group(**kwargs)</span>                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6615 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">describe_feature_group</span>(                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6617 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:6 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 02 in _api_call                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 599 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 600 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 601 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 603 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 604 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 605 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">context.py</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 123 in wrapper                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> start_as_current_context():                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> hook:                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hook()                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">func(*args, **kwargs)</span>                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:1 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 078 in _make_api_call                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1075 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">'error_code_override'</span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1076 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> error_info.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>)                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1077 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1078 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> error_class(parsed_response, operation_name)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1079 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1080 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1081 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ResourceInUse: </span>An error occurred <span style=\"font-weight: bold\">(</span>ResourceInUse<span style=\"font-weight: bold\">)</span> when calling the CreateFeatureGroup operation: Resource Already \n",
       "Exists: FeatureGroup with name wine_quality already exists. Choose a different name.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:2                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[2m# create feature store\u001b[0m                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[1;4mfeature_group.create(\u001b[0m                                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4ms3_uri=\u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33ms3://\u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mbucket\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m/features\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m,\u001b[0m                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mrecord_identifier_name = \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mid\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m,\u001b[0m                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mevent_time_feature_name = \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mEventTime\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m,\u001b[0m                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/feature_st\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2more/\u001b[0m\u001b[1mfeature_group.py\u001b[0m:789 in create                                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 786 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m{\u001b[33m\"\u001b[0m\u001b[33moffline_store_config\u001b[0m\u001b[33m\"\u001b[0m: offline_store_config.to_dict()}                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 787 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 788 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 789 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.create_feature_group(**create_feature_store_args)\u001b[0m   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 790 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 791 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mdelete\u001b[0m(\u001b[96mself\u001b[0m):                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 792 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Delete a FeatureGroup.\"\"\"\u001b[0m                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/\u001b[0m\u001b[1msession.py\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m :6614 in create_feature_group                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6611 \u001b[0m\u001b[2m│   │   │   \u001b[0mDescription=description,                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6612 \u001b[0m\u001b[2m│   │   │   \u001b[0mTags=tags,                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6613 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m6614 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_client.create_feature_group(**kwargs)\u001b[0m                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6615 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6616 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mdescribe_feature_group\u001b[0m(                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6617 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:6 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 02 in _api_call                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 599 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 600 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 601 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 602 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 603 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 604 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 605 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mcontext.py\u001b[0m: \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 123 in wrapper                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m start_as_current_context():                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m hook:                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhook()                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfunc(*args, **kwargs)\u001b[0m                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m126 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:1 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 078 in _make_api_call                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1075 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m'\u001b[0m\u001b[33merror_code_override\u001b[0m\u001b[33m'\u001b[0m                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1076 \u001b[0m\u001b[2m│   │   │   \u001b[0m) \u001b[95mor\u001b[0m error_info.get(\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m)                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1078 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m error_class(parsed_response, operation_name)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1079 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1080 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1081 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mResourceInUse: \u001b[0mAn error occurred \u001b[1m(\u001b[0mResourceInUse\u001b[1m)\u001b[0m when calling the CreateFeatureGroup operation: Resource Already \n",
       "Exists: FeatureGroup with name wine_quality already exists. Choose a different name.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create feature store\n",
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/features\",\n",
    "    record_identifier_name = 'id',\n",
    "    event_time_feature_name = \"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, ingest some data into your feature group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='wine_quality', feature_definitions={'alcohol': {'FeatureName': 'alcohol', 'FeatureType': 'Fractional'}, 'malic_acid': {'FeatureName': 'malic_acid', 'FeatureType': 'Fractional'}, 'ash': {'FeatureName': 'ash', 'FeatureType': 'Fractional'}, 'alcalinity_of_ash': {'FeatureName': 'alcalinity_of_ash', 'FeatureType': 'Fractional'}, 'magnesium': {'FeatureName': 'magnesium', 'FeatureType': 'Fractional'}, 'total_phenols': {'FeatureName': 'total_phenols', 'FeatureType': 'Fractional'}, 'flavanoids': {'FeatureName': 'flavanoids', 'FeatureType': 'Fractional'}, 'nonflavanoid_phenols': {'FeatureName': 'nonflavanoid_phenols', 'FeatureType': 'Fractional'}, 'proanthocyanins': {'FeatureName': 'proanthocyanins', 'FeatureType': 'Fractional'}, 'color_intensity': {'FeatureName': 'color_intensity', 'FeatureType': 'Fractional'}, 'hue': {'FeatureName': 'hue', 'FeatureType': 'Fractional'}, 'od280_od315_of_diluted_wines': {'FeatureName': 'od280_od315_of_diluted_wines', 'FeatureType': 'Fractional'}, 'proline': {'FeatureName': 'proline', 'FeatureType': 'Fractional'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}, 'id': {'FeatureName': 'id', 'FeatureType': 'Integral'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f3d5dde4890>, sagemaker_session=<sagemaker.session.Session object at 0x7f3dbc4b4c20>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f3d14303050>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingest data into feature group\n",
    "feature_group.ingest(data_frame=df, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You've demonstrated your understanding of creating feature groups and ingesting data into them using Feature Store. Next up we'll cover Model Monitor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll create a monitoring schedule for a deployed model. We're going to provide code to help you deploy a model and get started, so that you can focus on Model Monitor for this exercise. __Remember to clean up your model before you end a work session__. We'll provide some code at the end to help you clean up your model. We'll begin by reloading our data from the previous exercise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "df.rename(columns = {'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to put the target variable in the first column per the docs for our chosen algorithm: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TARGET\"] = data['target']\n",
    "df.set_index(df.pop('TARGET'), inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the data to S3 as train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = int(len(df)/2)\n",
    "train, test = df.iloc[delimiter:], df.iloc[:delimiter]\n",
    "\n",
    "train.to_csv(\"train.csv\", header=False, index=False)\n",
    "test.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "val_location = session.upload_data('./validation.csv', key_prefix=\"data\")\n",
    "train_location = session.upload_data('./train.csv', key_prefix=\"data\")\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-2026-01-25-20-59-30-301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 20:59:31 Starting - Starting the training job...\n",
      "2026-01-25 20:59:46 Starting - Preparing the instances for training...\n",
      "2026-01-25 21:00:11 Downloading - Downloading input data...\n",
      "2026-01-25 21:00:41 Downloading - Downloading the training image......\n",
      "2026-01-25 21:01:57 Training - Training image download completed. Training in progress.\n",
      "2026-01-25 21:01:57 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2026-01-25:21:01:52:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2026-01-25:21:01:52:INFO] File size need to be processed in the node: 0.01mb. Available memory size in the node: 8529.61mb\u001b[0m\n",
      "\u001b[34m[2026-01-25:21:01:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:01:52] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[21:01:52] 89x13 matrix with 1157 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2026-01-25:21:01:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:01:52] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[21:01:52] 89x13 matrix with 1157 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.9357#011validation-rmse:0.53422\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\u001b[0m\n",
      "\u001b[34mWill train until validation-rmse hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 2 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.759657#011validation-rmse:0.685354\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.616137#011validation-rmse:0.821641\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 2 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.501087#011validation-rmse:0.858446\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.430041#011validation-rmse:0.925116\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.377443#011validation-rmse:0.979923\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 4 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.337549#011validation-rmse:1.0265\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.309158#011validation-rmse:1.06425\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.287472#011validation-rmse:1.09787\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.273879#011validation-rmse:1.12296\u001b[0m\n",
      "\u001b[34m[21:01:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 6 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:0.264732#011validation-rmse:1.1432\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.9357#011validation-rmse:0.53422\u001b[0m\n",
      "\n",
      "2026-01-25 21:02:10 Completed - Training job completed\n",
      "Training seconds: 119\n",
      "Billable seconds: 119\n"
     ]
    }
   ],
   "source": [
    "algo_image = sagemaker.image_uris.retrieve(\"xgboost\", region, version='latest')\n",
    "s3_output_location = f\"s3://{bucket}/models/wine_model\"\n",
    "\n",
    "model=sagemaker.estimator.Estimator(\n",
    "    image_uri=algo_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "model.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n",
    "\n",
    "\n",
    "model.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your training job has finished, you can perform the first task in this exercise: creating a data capture config. Configure your model to sample `34%` of inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model capture\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "capture_uri = f's3://{bucket}/data-capture'\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri = capture_uri\n",
    ")\n",
    "# Model Capture in AWS SageMaker is a monitoring feature that automatically records data \n",
    "# passing through  deployed models in production\n",
    "# detect data drift and concept drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We'll use your config to deploy a model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2026-01-25-21-02-48-064\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-2026-01-25-21-02-48-064\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-2026-01-25-21-02-48-064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You should see an indicator like this when the deployment finishes:\n",
    "\n",
    "```\n",
    "-----------------!\n",
    "```\n",
    "We can test your deployment like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6030303239822388,0.6030303239822388,0.6030303239822388,0.6030303239822388,0.6030303239822388'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "inputs = test.copy()\n",
    "# Drop the target variable\n",
    "inputs = inputs.drop(columns=inputs.columns[0])\n",
    "x_pred = xgb_predictor.predict(inputs.sample(5).values).decode('utf-8')\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All systems go! To finish up the exercise, we're going to provide you with a DefaultModelMonitor and a suggested baseline. Combine the `xgb_predictor` and the provided `my_monitor` to configure the monitoring schedule for _hourly_ monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-01-25-21-06-20-536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2026-01-25 21:09:07.952737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:07.952772: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:09.366955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:09.366990: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:09.367016: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-181-234.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:09.367348: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,639 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:698775791571:processing-job/baseline-suggestion-job-2026-01-25-21-06-20-536', 'ProcessingJobName': 'baseline-suggestion-job-2026-01-25-21-06-20-536', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-698775791571/data/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-698775791571/model-monitor/baselining/baseline-suggestion-job-2026-01-25-21-06-20-536/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::698775791571:role/service-role/AmazonSageMaker-ExecutionRole-20251130T160736', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,640 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,640 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,640 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,640 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,640 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,707 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,708 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,708 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,718 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,718 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:10,718 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,210 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.181.234\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/h\u001b[0m\n",
      "\u001b[34madoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,219 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,224 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-e5bb4f1c-9bee-4047-ba17-ea3e22d12546\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,696 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,707 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,708 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,710 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,714 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,714 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,714 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,714 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,743 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,753 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,753 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,757 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,760 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Jan 25 21:09:11\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,761 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,761 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,762 INFO util.GSet: 2.0% max memory 3.1 GB = 62.8 MB\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,762 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,799 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,802 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,802 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,802 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,803 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,827 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,827 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,827 INFO util.GSet: 1.0% max memory 3.1 GB = 31.4 MB\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,827 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,830 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,830 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,830 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,830 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,834 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,838 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,838 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,838 INFO util.GSet: 0.25% max memory 3.1 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,838 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,870 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,870 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,870 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,873 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,873 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,874 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,874 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,875 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 964.0 KB\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,875 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,894 INFO namenode.FSImage: Allocated new BlockPoolId: BP-327888292-10.0.181.234-1769375351888\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,908 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,919 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:11,997 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:12,009 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:12,012 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.181.234\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:12,024 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:14,094 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:14,095 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:16,173 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:16,174 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:18,258 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:18,259 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:20,341 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:20,342 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:22,456 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:22,457 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:32,467 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:34,178 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:34,670 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:34,707 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:34,716 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,217 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,240 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,241 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,241 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,241 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,263 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11393, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,276 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,278 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,323 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,324 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,324 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,324 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,325 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,678 INFO util.Utils: Successfully started service 'sparkDriver' on port 46241.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,705 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,737 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,756 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,757 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,787 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,806 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-8a222ae6-555e-4d2f-b8cb-f6b28f16375d\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,821 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,856 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:35,887 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.181.234:46241/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1769375375213\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,308 INFO client.RMProxy: Connecting to ResourceManager at /10.0.181.234:8032\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,975 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,976 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,982 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15536 MB per container)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,982 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,982 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,983 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:36,988 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:37,062 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:39,249 INFO yarn.Client: Uploading resource file:/tmp/spark-58772a0c-8cc3-490d-ad3b-495ddb5fb977/__spark_libs__8346717539430866033.zip -> hdfs://10.0.181.234/user/root/.sparkStaging/application_1769375357658_0001/__spark_libs__8346717539430866033.zip\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,375 INFO yarn.Client: Uploading resource file:/tmp/spark-58772a0c-8cc3-490d-ad3b-495ddb5fb977/__spark_conf__4196478420236657609.zip -> hdfs://10.0.181.234/user/root/.sparkStaging/application_1769375357658_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,426 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,426 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,426 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,426 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,426 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,451 INFO yarn.Client: Submitting application application_1769375357658_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:40,643 INFO impl.YarnClientImpl: Submitted application application_1769375357658_0001\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:41,657 INFO yarn.Client: Application report for application_1769375357658_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:41,662 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sun Jan 25 21:09:41 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1769375380544\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1769375357658_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:42,665 INFO yarn.Client: Application report for application_1769375357658_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:43,669 INFO yarn.Client: Application report for application_1769375357658_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:44,671 INFO yarn.Client: Application report for application_1769375357658_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:45,675 INFO yarn.Client: Application report for application_1769375357658_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,186 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1769375357658_0001), /proxy/application_1769375357658_0001\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,679 INFO yarn.Client: Application report for application_1769375357658_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,680 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.181.234\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1769375380544\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1769375357658_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,682 INFO cluster.YarnClientSchedulerBackend: Application application_1769375357658_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,695 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43989.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,695 INFO netty.NettyBlockTransferService: Server created on 10.0.181.234:43989\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,698 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,707 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.181.234, 43989, None)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,711 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.181.234:43989 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.181.234, 43989, None)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,715 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.181.234, 43989, None)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,716 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.181.234, 43989, None)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:46,850 INFO util.log: Logging initialized @14108ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:47,520 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:50,574 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.181.234:42134) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:09:50,806 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:44435 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 44435, None)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:06,208 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:06,393 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:06,450 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:06,455 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:07,472 INFO datasources.InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:07,659 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:07,945 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:07,948 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.181.234:43989 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:07,954 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,327 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,330 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,333 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 6004\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,397 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,416 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,416 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,416 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,418 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,424 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,469 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,472 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,473 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.181.234:43989 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,474 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,491 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,492 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,534 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:08,764 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:44435 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,502 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:44435 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,786 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1264 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,788 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,794 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.347 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,797 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,797 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,799 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.401529 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,956 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.181.234:43989 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:09,960 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:44435 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:11,907 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:11,908 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:11,911 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 12 more fields>\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,092 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,106 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,107 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.181.234:43989 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,108 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,125 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,174 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,176 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,176 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,176 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,179 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,187 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,250 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,256 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,257 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.181.234:43989 (size: 8.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,257 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,258 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,259 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,263 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:12,313 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:44435 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,216 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:44435 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,324 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:44435 (size: 10.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,429 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1169 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,430 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,430 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.239 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,431 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,431 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,431 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.257185 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,687 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.181.234:43989 in memory (size: 8.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,692 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:44435 in memory (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:13,737 INFO codegen.CodeGenerator: Code generated in 244.721645 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,217 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,359 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,364 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,365 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,365 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,368 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,369 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,393 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 114.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,395 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,396 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.181.234:43989 (size: 34.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,397 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,399 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,399 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,408 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:14,433 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:44435 (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,795 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1389 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,795 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,797 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.424 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,797 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,798 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,798 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,799 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,882 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,885 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,885 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,885 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,885 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,886 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,899 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,902 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,903 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.181.234:43989 (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,903 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,904 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,904 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,907 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,927 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:44435 (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:15,990 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,450 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 544 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,450 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,451 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.559 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,453 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,453 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,454 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.571162 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,489 INFO codegen.CodeGenerator: Code generated in 24.44442 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,796 INFO codegen.CodeGenerator: Code generated in 33.855194 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,869 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,871 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,871 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,871 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,872 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,873 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,893 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,896 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,896 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.181.234:43989 (size: 16.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,897 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,898 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,898 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,900 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:16,926 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:44435 (size: 16.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,200 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 300 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,202 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.325 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,203 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,203 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,204 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,204 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.334838 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,684 INFO codegen.CodeGenerator: Code generated in 107.521343 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,693 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,693 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,694 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,694 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,694 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,696 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,702 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 74.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,705 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,705 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.181.234:43989 (size: 23.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,706 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,706 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,706 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,708 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,730 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:44435 (size: 23.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,827 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,827 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,828 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.131 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,828 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,829 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,830 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:17,830 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,092 INFO codegen.CodeGenerator: Code generated in 109.174556 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,105 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,106 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,106 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,106 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,107 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,108 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,111 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,116 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,116 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.181.234:43989 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,122 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,123 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,123 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,124 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,142 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:44435 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,148 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,259 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,259 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,261 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.151 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,262 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,263 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,263 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.158492 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,365 INFO codegen.CodeGenerator: Code generated in 77.634341 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,496 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,500 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,500 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,501 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,501 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,501 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,505 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,514 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,532 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,533 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.181.234:43989 (size: 13.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,534 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,534 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,534 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,537 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:18,565 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:44435 (size: 13.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,748 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1211 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,748 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,749 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.243 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,749 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,749 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,749 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,749 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,750 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,752 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,754 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,757 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.181.234:43989 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,758 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,759 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,759 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,766 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,778 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:44435 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,784 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,867 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 102 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,867 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,868 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.117 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,868 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,868 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:19,868 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.371749 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,038 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.181.234:43989 in memory (size: 23.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,043 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:44435 in memory (size: 23.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,069 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:44435 in memory (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,090 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.181.234:43989 in memory (size: 34.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,110 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,111 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,111 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,111 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,112 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,115 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,121 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 83.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,124 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,124 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.181.234:43989 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,125 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,125 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,126 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,127 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,135 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.181.234:43989 in memory (size: 16.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,142 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:44435 in memory (size: 16.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,144 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:44435 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,160 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.181.234:43989 in memory (size: 13.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,165 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:44435 in memory (size: 13.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,185 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.181.234:43989 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,191 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:44435 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,220 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.181.234:43989 in memory (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,223 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:44435 in memory (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,235 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:44435 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,237 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.181.234:43989 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,318 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 191 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,318 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,320 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.204 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,320 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,320 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,320 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,320 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,387 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,388 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,388 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,389 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,389 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,390 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,395 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,397 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,398 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.181.234:43989 (size: 46.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,398 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,399 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,399 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,400 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,411 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:44435 (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,420 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,532 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 132 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,533 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,533 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.142 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,533 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,533 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,534 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.146265 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,645 INFO codegen.CodeGenerator: Code generated in 14.627967 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,671 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,672 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,672 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,672 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,673 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,673 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,680 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,682 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,683 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.181.234:43989 (size: 16.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,683 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,683 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,684 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,685 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,694 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:44435 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,730 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,730 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,731 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.056 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,731 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,731 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,732 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.060621 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,923 INFO codegen.CodeGenerator: Code generated in 57.846429 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,931 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,931 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,932 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,932 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,933 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,933 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,938 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 73.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,940 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,941 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.181.234:43989 (size: 23.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,941 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,942 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,942 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,944 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:20,955 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:44435 (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,020 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,021 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,022 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,027 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,027 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,027 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,027 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,260 INFO codegen.CodeGenerator: Code generated in 126.880653 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,274 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,279 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,282 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,283 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,283 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,284 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,289 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,291 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,293 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.181.234:43989 (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,294 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,294 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,295 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,297 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,315 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:44435 (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,323 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,435 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,435 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,437 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.152 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,440 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,441 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,441 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.162777 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,533 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,534 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,535 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,536 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,536 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,537 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,538 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,549 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 30.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,552 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,552 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.181.234:43989 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,554 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,555 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,558 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,560 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,578 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:44435 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,648 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,648 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,650 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,651 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,652 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,652 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,652 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,653 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,655 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,657 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,659 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.181.234:43989 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,660 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,661 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,661 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,663 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,697 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:44435 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,714 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,772 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 110 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,772 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,773 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,773 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,773 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:21,773 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.240277 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,014 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,015 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,016 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,016 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,023 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,023 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,029 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 73.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,031 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,033 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.181.234:43989 (size: 25.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,034 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,035 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,035 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,036 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,052 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:44435 (size: 25.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,348 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 312 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,348 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,349 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.324 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,350 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,350 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,350 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,350 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,415 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,416 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,417 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,417 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,417 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,417 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,427 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 141.6 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,429 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,430 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.181.234:43989 (size: 40.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,430 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,431 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,431 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,432 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,442 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:44435 (size: 40.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,472 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,644 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 212 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,645 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,645 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.226 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,645 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,646 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,646 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.230204 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,668 INFO codegen.CodeGenerator: Code generated in 19.407779 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,781 INFO codegen.CodeGenerator: Code generated in 13.188763 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,811 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,812 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,812 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,812 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,813 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,813 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,820 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 37.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,822 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,823 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.181.234:43989 (size: 16.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,827 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,828 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,828 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,829 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,843 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:44435 (size: 16.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,879 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,879 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,879 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,879 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,880 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:22,880 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.068148 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,064 INFO codegen.CodeGenerator: Code generated in 44.819722 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,071 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,071 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,071 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,071 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,072 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,072 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,075 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 63.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,077 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,078 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.181.234:43989 (size: 20.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,079 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,081 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,081 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,082 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,103 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:44435 (size: 20.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,197 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 115 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,197 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,198 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.125 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,198 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,198 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,198 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,198 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,364 INFO codegen.CodeGenerator: Code generated in 85.380823 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,375 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,376 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,376 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,376 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,376 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,377 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,378 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 55.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,384 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,385 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.181.234:43989 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,387 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,387 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,388 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,390 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,400 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:44435 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,404 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,448 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 58 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,448 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,449 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,450 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,450 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,450 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.075305 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,536 INFO codegen.CodeGenerator: Code generated in 69.549475 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,586 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,587 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,587 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,588 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,588 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,588 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,592 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,597 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 30.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,602 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,603 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.181.234:43989 (size: 14.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,605 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,605 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,605 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,608 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,626 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:44435 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,708 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,708 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,709 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,709 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,709 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,709 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,709 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,710 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,711 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,738 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,742 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.181.234:43989 (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,743 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,744 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,744 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,745 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,762 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.181.234:43989 in memory (size: 19.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,767 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:44435 in memory (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,767 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:44435 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,771 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,793 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,793 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,793 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,794 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,794 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,794 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.208223 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,806 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.181.234:43989 in memory (size: 16.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,812 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:44435 in memory (size: 16.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,824 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.181.234:43989 in memory (size: 16.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,829 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:44435 in memory (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,863 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:44435 in memory (size: 40.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,872 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.181.234:43989 in memory (size: 40.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,888 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:44435 in memory (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,895 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.181.234:43989 in memory (size: 46.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,904 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.181.234:43989 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,907 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:44435 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,940 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.181.234:43989 in memory (size: 25.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,943 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:44435 in memory (size: 25.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,964 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.181.234:43989 in memory (size: 20.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,967 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:44435 in memory (size: 20.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,982 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.181.234:43989 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,989 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:44435 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:23,997 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,022 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.181.234:43989 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,024 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:44435 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,028 INFO codegen.CodeGenerator: Code generated in 9.739134 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,037 INFO scheduler.DAGScheduler: Registering RDD 121 (count at StatsGenerator.scala:66) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,037 INFO scheduler.DAGScheduler: Got map stage job 20 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,037 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,038 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,038 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,038 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,042 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,044 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.181.234:43989 in memory (size: 23.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,045 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,045 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.181.234:43989 (size: 10.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,046 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,046 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,046 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,047 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:44435 in memory (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,048 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,056 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:44435 (size: 10.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,069 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.181.234:43989 in memory (size: 14.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,076 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:44435 in memory (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,132 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,133 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,133 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (count at StatsGenerator.scala:66) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,134 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,134 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,134 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,135 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,153 INFO codegen.CodeGenerator: Code generated in 8.849895 ms\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,164 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,164 INFO scheduler.DAGScheduler: Got job 21 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,164 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,164 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,165 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,165 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,167 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,168 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,168 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.181.234:43989 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,169 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,169 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,170 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,171 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,180 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:44435 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,184 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.181.234:42134\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,198 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,198 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,199 INFO scheduler.DAGScheduler: ResultStage 31 (count at StatsGenerator.scala:66) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,199 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,199 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,200 INFO scheduler.DAGScheduler: Job 21 finished: count at StatsGenerator.scala:66, took 0.036054 s\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,371 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,428 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,449 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,449 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,454 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,474 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,506 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,506 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,509 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,512 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,591 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,592 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,592 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,609 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,611 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-58772a0c-8cc3-490d-ad3b-495ddb5fb977\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,619 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7e87ebad-7551-4612-a723-26db17648983\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,706 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-01-25 21:10:24,707 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f3d7d5a9820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset=f's3://{bucket}/data/train.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, provide the monitoring schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:sagemaker.model_monitor.model_monitoring:It seems that this object was already used to create an Amazon Model Monitoring Schedule. To create another, first delete the existing one using my_monitor.delete_monitoring_schedule().\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:3                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">sagemaker.model_monitor</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> CronExpressionGenerator                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3 <span style=\"font-weight: bold; text-decoration: underline\">my_monitor.create_monitoring_schedule(</span>                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">monitor_schedule_name=</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"my-monitoring-schedule\"</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">endpoint_input=xgb_predictor.endpoint_name,</span>                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">statistics=my_monitor.baseline_statistics(),</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/model_moni</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tor/</span><span style=\"font-weight: bold\">model_monitoring.py</span>:1998 in create_monitoring_schedule                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1995 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"using my_monitor.delete_monitoring_schedule().\"</span>                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1996 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1997 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>logger.error(message)                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1998 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">ValueError</span><span style=\"font-weight: bold; text-decoration: underline\">(message)</span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1999 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2000 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (batch_transform_input <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) ^ (endpoint_input <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2001 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>message = (                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>It seems that this object was already used to create an Amazon Model Monitoring Schedule. To create \n",
       "another, first delete the existing one using <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-weight: bold\">my_monitor.delete_monitoring_schedule</span><span style=\"font-weight: bold\">()</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:3                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mfrom\u001b[0m\u001b[90m \u001b[0m\u001b[4;96msagemaker\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodel_monitor\u001b[0m\u001b[90m \u001b[0m\u001b[94mimport\u001b[0m CronExpressionGenerator                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 2 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 3 \u001b[1;4mmy_monitor.create_monitoring_schedule(\u001b[0m                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mmonitor_schedule_name=\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mmy-monitoring-schedule\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m,\u001b[0m                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mendpoint_input=xgb_predictor.endpoint_name,\u001b[0m                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mstatistics=my_monitor.baseline_statistics(),\u001b[0m                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/model_moni\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2mtor/\u001b[0m\u001b[1mmodel_monitoring.py\u001b[0m:1998 in create_monitoring_schedule                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1995 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33musing my_monitor.delete_monitoring_schedule().\u001b[0m\u001b[33m\"\u001b[0m                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1996 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1997 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogger.error(message)                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1998 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(message)\u001b[0m                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1999 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2000 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m (batch_transform_input \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m) ^ (endpoint_input \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m):                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2001 \u001b[0m\u001b[2m│   │   │   \u001b[0mmessage = (                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mIt seems that this object was already used to create an Amazon Model Monitoring Schedule. To create \n",
       "another, first delete the existing one using \u001b[1;38;2;225;0;225mmy_monitor.delete_monitoring_schedule\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"my-monitoring-schedule\",\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You can check that your schedule was created by selecting the `SageMaker components and registries` tab on the far left.\n",
    "\n",
    "In this exercise you configured Model Monitor to watch a simple model. Next, we'll monitor the same deployment for explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REMINDER:__ Don't leave your model deployed overnight. If you aren't going to follow up with the Clarify exercise within a few hours, use the code below to remove your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Deleting Monitoring Schedule with name: my-monitoring-schedule\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2026-01-25-21-11-44-582\n"
     ]
    }
   ],
   "source": [
    "monitors = xgb_predictor.list_monitors()\n",
    "for monitor in monitors:\n",
    "    monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: xgboost-2026-01-25-21-02-48-064\n",
      "INFO:sagemaker:Deleting endpoint with name: xgboost-2026-01-25-21-02-48-064\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarify\n",
    "\n",
    "For the last exercise we'll deploy an explainability monitor using Clarify. We're going to use the model that you deployed in the last exercise, but if you cleaned up your deployments from the previous exercise, that's ok! You can rerun the deployment from the previous exercise up to the point where we deployed our model. It'll look like this:\n",
    "\n",
    "```python\n",
    "xgb_predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")\n",
    "```\n",
    "\n",
    "Once your model is deployed, you can come back here. _REMINDER_: you need to clean up your deployment, don't leave it running overnight. We'll provide some code at the end to delete your deployment.\n",
    "\n",
    "## Prep\n",
    "\n",
    "We'll begin by reloading our data from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "df.rename(columns = {'od280/od315_of_diluted_wines':'od280_od315_of_diluted_wines'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to put the target variable in the first column per the docs for our chosen algorithm: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TARGET\"] = data['target']\n",
    "df.set_index(df.pop('TARGET'), inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the data to S3 as train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = int(len(df)/2)\n",
    "train, test = df.iloc[delimiter:], df.iloc[:delimiter]\n",
    "\n",
    "train.to_csv(\"train.csv\", header=False, index=False)\n",
    "test.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "val_location = session.upload_data('./validation.csv', key_prefix=\"data\")\n",
    "train_location = session.upload_data('./train.csv', key_prefix=\"data\")\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our data is staged and our model is deployed - let's monitor it for explainability. We need to define three config objects, the `SHAPConfig`, the `ModelConfig`, and the `ExplainabilityAnalysisConfig`. Below, we provide the `SHAPConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    baseline=[train.mean().astype(int).to_list()[1:]],\n",
    "    num_samples=int(train.size),\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, fill in the blanks to define the `ModelConfig` and `ExplainabilityAnalysisConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=xgb_predictor.endpoint_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type='text/csv',\n",
    "    accept_type='text/csv'\n",
    ")\n",
    "\n",
    "analysis_config = sagemaker.model_monitor.ExplainabilityAnalysisConfig(\n",
    "    explainability_config=shap_config,\n",
    "    model_config=model_config,\n",
    "    headers=train.columns.to_list()[1:]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply our config, we need to create the monitor object. This is what we'll apply all our config to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor = sagemaker.model_monitor.ModelExplainabilityMonitor(\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything's ready! Below, create a monitoring schedule using the configs we created. Set the schedule to run _daily_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-2026-01-25-21-02-48-064\n"
     ]
    }
   ],
   "source": [
    "print(xgb_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.clarify_model_monitoring:Uploading analysis config to {s3_uri}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:5                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>explainability_uri = <span style=\"color: #808000; text-decoration-color: #808000\">f\"s3://{</span>bucket<span style=\"color: #808000; text-decoration-color: #808000\">}/model_explainability\"</span>                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 <span style=\"font-weight: bold; text-decoration: underline\">model_explainability_monitor.create_monitoring_schedule(</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">output_s3_uri=explainability_uri,</span>                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">analysis_config=analysis_config,</span>                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">endpoint_input=xgb_predictor.endpoint_name,</span>                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/model_moni</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tor/</span><span style=\"font-weight: bold\">clarify_model_monitoring.py</span>:1096 in create_monitoring_schedule                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1093 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>network_config=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.network_config,                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1094 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_transform_input=batch_transform_input,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1095 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1096 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.sagemaker_client.create_model_explainability_job_definiti</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1097 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">**request_dict</span>                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1098 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:6 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 02 in _api_call                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 599 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 600 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 601 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 602 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 603 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 604 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 605 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">context.py</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 123 in wrapper                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> start_as_current_context():                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> hook:                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hook()                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">func(*args, **kwargs)</span>                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:1 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> 078 in _make_api_call                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1075 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">'error_code_override'</span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1076 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> error_info.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>)                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1077 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1078 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> error_class(parsed_response, operation_name)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1079 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1080 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1081 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ClientError: </span>An error occurred <span style=\"font-weight: bold\">(</span>ValidationException<span style=\"font-weight: bold\">)</span> when calling the CreateModelExplainabilityJobDefinition \n",
       "operation: Endpoint <span style=\"color: #008700; text-decoration-color: #008700\">'xgboost-2026-01-25-21-02-48-064'</span> does not exist or is not valid\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:5                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 2 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 3 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 4 \u001b[0mexplainability_uri = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33ms3://\u001b[0m\u001b[33m{\u001b[0mbucket\u001b[33m}\u001b[0m\u001b[33m/model_explainability\u001b[0m\u001b[33m\"\u001b[0m                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 5 \u001b[1;4mmodel_explainability_monitor.create_monitoring_schedule(\u001b[0m                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4moutput_s3_uri=explainability_uri,\u001b[0m                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4manalysis_config=analysis_config,\u001b[0m                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mendpoint_input=xgb_predictor.endpoint_name,\u001b[0m                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/sagemaker/model_moni\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2mtor/\u001b[0m\u001b[1mclarify_model_monitoring.py\u001b[0m:1096 in create_monitoring_schedule                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1093 \u001b[0m\u001b[2m│   │   │   \u001b[0mnetwork_config=\u001b[96mself\u001b[0m.network_config,                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1094 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_transform_input=batch_transform_input,                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1095 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1096 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.sagemaker_client.create_model_explainability_job_definiti\u001b[0m  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1097 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4m**request_dict\u001b[0m                                                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1098 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1099 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:6 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 02 in _api_call                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 599 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 600 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 601 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 602 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 603 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 604 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 605 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mcontext.py\u001b[0m: \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 123 in wrapper                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m start_as_current_context():                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m hook:                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhook()                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfunc(*args, **kwargs)\u001b[0m                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m126 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:1 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m 078 in _make_api_call                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1075 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m'\u001b[0m\u001b[33merror_code_override\u001b[0m\u001b[33m'\u001b[0m                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1076 \u001b[0m\u001b[2m│   │   │   \u001b[0m) \u001b[95mor\u001b[0m error_info.get(\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m)                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1078 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m error_class(parsed_response, operation_name)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1079 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1080 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1081 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mClientError: \u001b[0mAn error occurred \u001b[1m(\u001b[0mValidationException\u001b[1m)\u001b[0m when calling the CreateModelExplainabilityJobDefinition \n",
       "operation: Endpoint \u001b[38;2;0;135;0m'xgboost-2026-01-25-21-02-48-064'\u001b[0m does not exist or is not valid\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "\n",
    "explainability_uri = f\"s3://{bucket}/model_explainability\"\n",
    "model_explainability_monitor.create_monitoring_schedule(\n",
    "    output_s3_uri=explainability_uri,\n",
    "    analysis_config=analysis_config,\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to go! You can check that your schedule was created by selecting the `SageMaker components and registries` tab on the far left.\n",
    "\n",
    "In this exercise you deployed a monitor for explainability to your SageMaker endpoint. This is the last exercise - you'll apply these learnings again in your Project at the end of the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REMINDER:__ Don't leave your model deployed overnight. Use the code below to remove your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitors = xgb_predictor.list_monitors()\n",
    "for monitor in monitors:\n",
    "    monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
