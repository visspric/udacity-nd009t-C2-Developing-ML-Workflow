{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19522d16",
   "metadata": {},
   "source": [
    "# UDACITY SageMaker Essentials: Processing Job Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf65b8e",
   "metadata": {},
   "source": [
    "In prior exercises, we've been running and rerunning the same preprocessing job over and over again. For cleanly formatted data, it's possible to do some preprocessing utilizing batch transform. However, if slightly more sophisticated processing is needed, we would want to do so through a processing job. Finally, after beating around the bush for a few exercises, we're finally going offload the preprocessing step of our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23acbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734aa859",
   "metadata": {},
   "source": [
    "## Preprocessing (for the final time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d50dd",
   "metadata": {},
   "source": [
    "The cell below should be very familiar to you by now. This cell will write the preprocessing code to a file called \"HelloBlazePreprocess.py\". This code will be utilized by AWS via a SciKitLearn processing interface to process our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc0742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HelloBlazePreprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HelloBlazePreprocess.py\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "        return input_data_zip.namelist()[0]\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    new_split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]        \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                new_split_sentences.append(\" \".join([label, s]))\n",
    "    return new_split_sentences\n",
    "\n",
    "def write_data(data, train_path, test_path, proportion):\n",
    "    border_index = int(proportion * len(data))\n",
    "    train_f = open(train_path, 'w')\n",
    "    test_f = open(test_path, 'w')\n",
    "    index = 0\n",
    "    for d in data:\n",
    "        if index < border_index:\n",
    "            train_f.write(d + '\\n')\n",
    "        else:\n",
    "            test_f.write(d + '\\n')\n",
    "        index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unzipped_path = unzip_data('/opt/ml/processing/input/Toys_and_Games_5.json.zip')\n",
    "    labeled_data = label_data(unzipped_path)\n",
    "    new_split_sentence_data = split_sentences(labeled_data)\n",
    "    write_data(new_split_sentence_data, '/opt/ml/processing/output/train/hello_blaze_train_scikit', '/opt/ml/processing/output/test/hello_blaze_test_scikit', .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1495005",
   "metadata": {},
   "source": [
    "## Exercise: Upload unprocessed data to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dd564",
   "metadata": {},
   "source": [
    "No more local preprocessing! Upload the **raw zipped** Toys_and_Games dataset to s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ff010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bostonhousedata3011/l2e4/Toys_and_Games_5.json.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3_bucket = \"bostonhousedata3011\"\n",
    "s3_prefix = \"l2e4\"\n",
    "file_name = \"Toys_and_Games_5.json.zip\"\n",
    "\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, s3_bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "upload_file_to_s3(file_name, s3_prefix)\n",
    "\n",
    "source_path = \"s3://\" + \"/\".join([s3_bucket, s3_prefix, file_name])\n",
    "print(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84ac2",
   "metadata": {},
   "source": [
    "## Exercise: Launch a processing job through the SciKitLearn interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ce1a3",
   "metadata": {},
   "source": [
    "We'll be utilizing the SKLearnProcessor object from SageMaker to launch a processing job. Here is some information you'll need to complete the exercise: \n",
    "\n",
    "* You will need to use the SKLearnProcessor object. \n",
    "* You will need 1 instance of ml.m5.large. You can specify this programatically. \n",
    "* You will need an execution role.  \n",
    "\n",
    "* You will need to call the \"run\" method on the SKLearnProcessor object.\n",
    "> * You will need to specify the preprocessing code\n",
    "> * the S3 path of the unprocessed data\n",
    "> * a 'local' directory path for the input to be downloaded into\n",
    "> * 'local' directory paths for where you expect the output to be.\n",
    "\n",
    "you will need to make use of the ProcessingInput and ProcessingOutput features. Review the preprocessing code for where the output is going to go, and where it expects the input to come from.  \n",
    "* It is highly recommended that you consult the documentation to help you implement this. https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\n",
    "\n",
    "Remember that, conceptually, you are creating a server that our code will be run from. This server will download data, execute code that we specify, and upload data to s3. \n",
    "\n",
    "If done successfully, you should see a processing job launch in the SageMaker console. To see it, go to the \"processing\" drop-down menu on the left-hand side and select \"processing jobs.\" Wait until the job is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dcf966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2025-12-07-19-17-27-851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      ".."
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Get role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create an SKLearnProcessor. Set framework_version='0.20.0'.\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version = '0.20.0', \n",
    "    role = role,\n",
    "    instance_type = 'ml.m5.large',\n",
    "    instance_count= 1\n",
    ")\n",
    "\n",
    "# Start a run job. You will pass in as parameters the local location of the processing code, \n",
    "# a processing input object, two processing output objects. The paths that you pass in here are directories, \n",
    "# not the files themselves. Check the preprocessing code for a hint about what these directories should be. \n",
    "\n",
    "sklearn_processor.run(code= 'HelloBlazePreprocess.py', # preprocessing code\n",
    "                      inputs=[ProcessingInput(\n",
    "                          source = source_path, # the S3 path of the unprocessed data\n",
    "                          destination= '/opt/ml/processing/input', # a 'local' directory path for the input to be downloaded into\n",
    "                      )],\n",
    "                      outputs=[ProcessingOutput(source= '/opt/ml/processing/output/train'),# a 'local' directory path for where you expect the output for train data to be\n",
    "                               ProcessingOutput(source= '/opt/ml/processing/output/test')]) # a 'local' directory path for where you expect the output for test data to be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31b0fb",
   "metadata": {},
   "source": [
    "## Exercise: Sanity Check & Reflection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05575057",
   "metadata": {},
   "source": [
    "If all goes well, processed data should have been uploaded to S3. If you're having trouble locating the uri, check the `jobs` attribute of the SKLearnProcessor object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d338b89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'input-1',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://bostonhousedata3011/l2e4/Toys_and_Games_5.json.zip',\n",
       "    'LocalPath': '/opt/ml/processing/input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'code',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-698775791571/sagemaker-scikit-learn-2025-12-07-19-17-27-851/input/code/HelloBlazePreprocess.py',\n",
       "    'LocalPath': '/opt/ml/processing/input/code',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-698775791571/sagemaker-scikit-learn-2025-12-07-19-17-27-851/output/output-1',\n",
       "     'LocalPath': '/opt/ml/processing/output/train',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False},\n",
       "   {'OutputName': 'output-2',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-698775791571/sagemaker-scikit-learn-2025-12-07-19-17-27-851/output/output-2',\n",
       "     'LocalPath': '/opt/ml/processing/output/test',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'sagemaker-scikit-learn-2025-12-07-19-17-27-851',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.large',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3',\n",
       "  'ContainerEntrypoint': ['python3',\n",
       "   '/opt/ml/processing/input/code/HelloBlazePreprocess.py']},\n",
       " 'RoleArn': 'arn:aws:iam::698775791571:role/service-role/AmazonSageMaker-ExecutionRole-20251130T160736',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:698775791571:processing-job/sagemaker-scikit-learn-2025-12-07-19-17-27-851',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2025, 12, 7, 19, 19, 41, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2025, 12, 7, 19, 18, 6, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 12, 7, 19, 20, 0, 47000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2025, 12, 7, 19, 17, 28, 234000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '52bae59d-b64f-4846-8d5f-ca159ef99afd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '52bae59d-b64f-4846-8d5f-ca159ef99afd',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1991',\n",
       "   'date': 'Sun, 07 Dec 2025 19:27:57 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2750ddf",
   "metadata": {},
   "source": [
    "Download these datasets and compare them to the datasets that we locally processed. The exact sentences in the training & the test sets may vary depending on your implementation, but the same number of sentences should be present in each job, and there should be one label and one sentence per line.  \n",
    "\n",
    "\n",
    "Once you've confirmed that the data was accurately processed, take a step back and reflect on what you've done. You've created the individual components necessary to process data, train data, and perform inference on both individual instances of data and large datasets. What are we missing if we wanted to provide a live, continuous service? Keep this question in mind as we move on to designing workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe7743a-2c3b-4fea-9d16-48579247c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1  Trading doesn't do you any good if you have nothing to trade\n",
      "__label__1  I cannot tell you how many times I have rolled the dice in Catan on MY turn and everyone gets a resource EXCEPT ME!! That, to me, is so frustrating it isn't fun\n",
      "__label__1  In Stone Age, you get to roll one dice for every worker in a particular area\n",
      "__label__1  For instance, if you have 3 workers in the clay pit, you would roll 3 dice\n",
      "__label__1  If you roll a 5, 3, 2, you note the SUM\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-698775791571\"\n",
    "key = \"sagemaker-scikit-learn-2025-12-07-19-17-27-851/output/output-2/hello_blaze_test_scikit\"\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "# Read first 5 lines\n",
    "lines = obj[\"Body\"].iter_lines()\n",
    "\n",
    "top5 = []\n",
    "for _ in range(5):\n",
    "    try:\n",
    "        top5.append(next(lines).decode(\"utf-8\"))\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "for line in top5:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28739db1-2101-47cb-93f3-1a4145206c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
